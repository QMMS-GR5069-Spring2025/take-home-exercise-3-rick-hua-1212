{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44a0f13-6795-48d0-9a97-d35d471108e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "116b328b-7089-4790-aea0-ad9b904760d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 加载 CSV 数据\n",
    "\n",
    "# df_races = spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header=True)\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header=True)\n",
    "# df_driver = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header=True)\n",
    "# df_pit_stops = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b87b542-8f89-4ed4-a3c7-c56b05960a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7771840-48fc-44c2-940c-f09bccb52464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove rows with null values\n",
    "df_results = df_results.dropna(subset=[\n",
    "    \"grid\", \"points\", \"laps\", \"milliseconds\", \"fastestLap\", \"rank\", \"fastestLapSpeed\", \"positionOrder\"\n",
    "])\n",
    "\n",
    "df_results = df_results.select(\n",
    "    col(\"grid\").cast(\"double\"),\n",
    "    col(\"points\").cast(\"double\"),\n",
    "    col(\"laps\").cast(\"double\"),\n",
    "    col(\"milliseconds\").cast(\"double\"),\n",
    "    col(\"fastestLap\").cast(\"double\"),\n",
    "    col(\"rank\").cast(\"double\"),\n",
    "    col(\"fastestLapSpeed\").cast(\"double\"),\n",
    "    col(\"positionOrder\").cast(\"double\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa64dc0c-508b-49b1-bf82-1412be9b0fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 选择特征和标签列\n",
    "feature_cols = ['grid', 'points', 'laps', 'milliseconds', 'fastestLap', 'rank', 'fastestLapSpeed']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "data = assembler.transform(df_results).select(\"features\", \"positionOrder\")\n",
    "\n",
    "# 拆分训练集和测试集\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2191796c-1972-4495-b3f3-fa8cbc8dfdb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import os\n",
    "\n",
    "# 创建本地目录存放 artifact\n",
    "os.makedirs(\"./artifacts\", exist_ok=True)\n",
    "\n",
    "# 设置 MLflow 实验名\n",
    "experiment_path = \"/Users/rh3243@columbia.edu/f1_spark_exp\"  # 替换成你自己的路径\n",
    "\n",
    "# 创建实验目录\n",
    "dbutils.fs.mkdirs(experiment_path)\n",
    "\n",
    "# 设置实验\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d8330d-abe3-4060-be81-71931c640eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    numTrees = int(10 + i * 5)\n",
    "    maxDepth = int(3 + i)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"positionOrder\",\n",
    "            numTrees=numTrees,\n",
    "            maxDepth=maxDepth,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        model = rf.fit(train_data)\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        # 评估指标\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"positionOrder\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        acc = evaluator.evaluate(predictions)\n",
    "\n",
    "        # 记录超参数和指标\n",
    "        mlflow.log_param(\"numTrees\", numTrees)\n",
    "        mlflow.log_param(\"maxDepth\", maxDepth)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "        # 记录模型\n",
    "        mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "        # 保存预测结果为 artifact（csv）\n",
    "        pred_pd = predictions.select(\"positionOrder\", \"prediction\").toPandas()\n",
    "        pred_path = f\"./artifacts/pred_{i}.csv\"\n",
    "        pred_pd.to_csv(pred_path, index=False)\n",
    "        mlflow.log_artifact(pred_path)\n",
    "\n",
    "        # 混淆矩阵图\n",
    "        cm = pd.crosstab(pred_pd[\"positionOrder\"], pred_pd[\"prediction\"], rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "        cm_fig = plt.figure()\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.imshow(cm, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plot_path = f\"./artifacts/cm_{i}.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        mlflow.log_artifact(plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b0129c-11f0-4ed7-8075-a462313ec4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. [20 pts] Select your best model run and explain why\n",
    "\n",
    "best model hyperparameters has maxDepth 12, numTrees 55, it has highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "690bcafd-d5d7-4660-b5ca-a3a8b4c814e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. [20 pts] As part of your GitHub classroom submission include screenshots\n",
    "\n",
    "in dir `screenshots`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35e4cd4-c18d-479f-986b-0dd971253192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
